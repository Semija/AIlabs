{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №6\n",
    "## Прогноз успеха фильмов по обзорам\n",
    "## Группа: БФИ1901"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цель работы:\n",
    "Реализовать прогноз успеха фильмов по обзорам (Predict Sentiment From Movie Reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание:\n",
    "* Ознакомиться с задачей классификации\n",
    "* Изучить способы представления текста для передачи в ИНС\n",
    "* Достигнуть точность прогноза не менее 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ход работы:\n",
    "#### Импорт зависимостей и получение данных\n",
    "Начнем с импорта необходимых зависимостей для предварительной обработки данных и\n",
    "построения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датесет IMDb, который уже встроен в Keras. Поскольку мы не хотим иметь данные обучения и тестирования в пропорции 50/50, мы сразу же объединим эти данные после загрузки для последующего разделения в пропорции 80/20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Изучение данных\n",
    "Изучим наш датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [0 1]\n",
      "Number of unique words: 9998\n",
      "Average Review length: 234.75892\n",
      "Standard Deviation: 173\n",
      "Label: 1\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(\"Categories:\", np.unique(targets))\n",
    "print(\"Number of unique words:\",\n",
    "len(np.unique(np.hstack(data))))\n",
    "\n",
    "length = [len(i) for i in data]\n",
    "print(\"Average Review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))\n",
    "\n",
    "print(\"Label:\", targets[0])\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нижеследующий код производит обратное преобразование индексов в слова, чтобы мы могли их прочесть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка данных\n",
    "Пришло время подготовить данные. Нужно векторизовать каждый обзор и заполнить его нулями, чтобы вектор содержал ровно 10 000 чисел. Это означает, что каждый обзор, который короче 10 000 слов, мы заполняем нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим датасет на обучающий и тестировочный наборы. Обучающий набор будет состоять из 40 000 обзоров, тестировочный — из 10 000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = data[:10000]\n",
    "test_y = targets[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = targets[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание и обучение модели\n",
    "Ниже представлен код, ответственный за построение, компиляцию и запуск модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                500050    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 505,201\n",
      "Trainable params: 505,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "80/80 [==============================] - 2s 17ms/step - loss: 0.4196 - accuracy: 0.8112 - val_loss: 0.2660 - val_accuracy: 0.8924\n",
      "Epoch 2/2\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.2186 - accuracy: 0.9165 - val_loss: 0.2607 - val_accuracy: 0.8968\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "# Input - Layer\n",
    "model.add(Dense(50, activation = \"relu\",input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "model.add(Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "    \n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "   \n",
    "results = model.fit(train_x, train_y, epochs= 2, batch_size = 500, validation_data = (test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос 1: Построить и обучить нейронную сеть для обработки текста\n",
    "Ниже представлен полный код программы, по которому можно проверить результат работы нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 50)                500050    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 505,201\n",
      "Trainable params: 505,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.8250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 3s 66us/sample - loss: 0.3974 - accuracy: 0.8250 - val_loss: 0.2720 - val_accuracy: 0.8915\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 2s 56us/sample - loss: 0.2153 - accuracy: 0.9166 - val_loss: 0.2652 - val_accuracy: 0.8934\n",
      "0.8924500048160553\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def vectorize(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets),axis=0)\n",
    "\n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")\n",
    "\n",
    "test_x = data[:10000]\n",
    "test_y = targets[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = targets[10000:]\n",
    "\n",
    "model = Sequential()\n",
    "# Input - Layer\n",
    "model.add(Dense(50, activation = \"relu\",input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "model.add(Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "    \n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "   \n",
    "results = model.fit(train_x, train_y, epochs= 2, batch_size = 500, validation_data = (test_x, test_y))\n",
    "\n",
    "print(np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос 2: Исследовать результаты при различном размере вектора представления текста\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Попробуем взять вектор размером в 5000 символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 50)                250050    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 255,201\n",
      "Trainable params: 255,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.4336 - accuracy: 0.7993 - val_loss: 0.2713 - val_accuracy: 0.8913\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.2460 - accuracy: 0.9034 - val_loss: 0.2652 - val_accuracy: 0.8914\n",
      "0.8913500010967255\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(sequences, dimension = 5000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=5000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets),axis=0)\n",
    "\n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")\n",
    "\n",
    "test_x = data[:10000]\n",
    "test_y = targets[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = targets[10000:]\n",
    "\n",
    "model = Sequential()\n",
    "# Input - Layer\n",
    "model.add(Dense(50, activation = \"relu\",input_shape=(5000, )))\n",
    "# Hidden - Layers\n",
    "model.add(Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "    \n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "   \n",
    "results = model.fit(train_x, train_y, epochs= 2, batch_size = 500, validation_data = (test_x, test_y))\n",
    "\n",
    "print(np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 50)                1000050   \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,005,201\n",
      "Trainable params: 1,005,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 5s 127us/sample - loss: 0.3964 - accuracy: 0.8264 - val_loss: 0.2537 - val_accuracy: 0.8981\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 4s 109us/sample - loss: 0.1892 - accuracy: 0.9302 - val_loss: 0.2621 - val_accuracy: 0.8977\n",
      "0.8979000151157379\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(sequences, dimension = 20000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=20000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets),axis=0)\n",
    "\n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")\n",
    "\n",
    "test_x = data[:10000]\n",
    "test_y = targets[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = targets[10000:]\n",
    "\n",
    "model = Sequential()\n",
    "# Input - Layer\n",
    "model.add(Dense(50, activation = \"relu\",input_shape=(20000, )))\n",
    "# Hidden - Layers\n",
    "model.add(Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "    \n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "   \n",
    "results = model.fit(train_x, train_y, epochs= 2, batch_size = 500, validation_data = (test_x, test_y))\n",
    "\n",
    "print(np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, увеличение размера вектора свыше 10000 не производит заметных изменений в эффективности обучения модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос 3: Написать функцию, которая позволяет ввести пользовательский текст (в отчете привести пример работы сети на пользовательском тексте)\n",
    "Ниже представлен код функции, пользволяющей вводить пользовательский текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "\n",
    "def input_user_text():\n",
    "    user_text = input().split()\n",
    "\n",
    "    index = imdb.get_word_index()\n",
    "    reverse_index = dict([(key, value) for (key, value) in index.items()])\n",
    "\n",
    "    conv_text_arr = []\n",
    "\n",
    "    for word in user_text:\n",
    "        conv_text_arr.append(reverse_index.get(word, 0))\n",
    "\n",
    "    result = np.array([conv_text_arr])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже представлен код основной программы, которая использует пользовательский текст для тестирования модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice movie\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 50)                500050    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 505,201\n",
      "Trainable params: 505,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 7s 175us/sample - loss: 0.3902 - accuracy: 0.8300 - val_loss: 0.2600 - val_accuracy: 0.8944\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 7s 169us/sample - loss: 0.2097 - accuracy: 0.9200 - val_loss: 0.2625 - val_accuracy: 0.8927\n",
      "0.565898\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets),axis=0)\n",
    "\n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")\n",
    "\n",
    "test_x = data[:10000]\n",
    "test_y = targets[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = targets[10000:]\n",
    "\n",
    "user_text = input_user_text()\n",
    "user_text = vectorize(user_text)\n",
    "\n",
    "model = Sequential()\n",
    "# Input - Layer\n",
    "model.add(Dense(50, activation = \"relu\",input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "model.add(Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()\n",
    "    \n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "   \n",
    "results = model.fit(train_x, train_y, epochs= 2, batch_size = 500, validation_data = (test_x, test_y))\n",
    "\n",
    "prediction = model.predict(user_text)[0][0]\n",
    "print(prediction)\n",
    "\n",
    "if prediction >= 0.5:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
