{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №8\n",
    "## Генерация текста на основе “Алисы в стране чудес”\n",
    "## Группа: БФИ1901"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цель работы:\n",
    "Создать генеративную модель для создания текста на основе \"Алисы в стране чудес\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание:\n",
    "* Ознакомиться с генерацией текста\n",
    "* Ознакомиться с системой Callback в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ход работы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос 1: Реализовать модель ИНС, которая будет генерировать текст\n",
    "Ниже представлен полный код нашей программы, которая создает модель для генерации текста.\n",
    "\n",
    "#### Импорт зависимостей\n",
    "Начнем с импорта необходимых зависимостей для предварительной обработки данных и построения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Получение и обработка данных\n",
    "Затем нам нужно загрузить текст ASCII для книги в память и преобразовать все символы в нижний регистр, чтобы уменьшить словарный запас, который должна выучить сеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда книга загружена, мы должны подготовить данные для моделирования нейронной сетью. Создадим набор всех отдельных символов в книге, а затем создадим карту каждого символа с уникальным целым числом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда книга загружена и карта подготовлена, мы можем суммировать набор данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144431\n",
      "Total Vocab:  45\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделяя книгу на эти последовательности, мы конвертируем символы в целые числа, используя нашу таблицу поиска, которую мы подготовили ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144331\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы подготовили наши тренировочные данные, нам нужно преобразовать их так, чтобы они подходили для использования с Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "X\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Настройка и обучение модели\n",
    "Теперь мы можем определить нашу модель LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из-за медлительности данной модели и из-за наших требований по оптимизации мы будем использовать контрольные точки модели для записи всех сетевых весов, чтобы каждый раз регистрировать улучшение потерь в конце эпохи. Мы будем использовать лучший набор весов (наименьшая потеря), чтобы реализовать нашу генеративную модель в следующем разделе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:03d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь осталось обучить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.9786\n",
      "Epoch 1: loss improved from inf to 2.97858, saving model to weights-improvement-001-2.9786.hdf5\n",
      "1128/1128 [==============================] - 283s 249ms/step - loss: 2.9786\n",
      "Epoch 2/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.7720\n",
      "Epoch 2: loss improved from 2.97858 to 2.77205, saving model to weights-improvement-002-2.7720.hdf5\n",
      "1128/1128 [==============================] - 286s 254ms/step - loss: 2.7720\n",
      "Epoch 3/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.6677\n",
      "Epoch 3: loss improved from 2.77205 to 2.66770, saving model to weights-improvement-003-2.6677.hdf5\n",
      "1128/1128 [==============================] - 288s 255ms/step - loss: 2.6677\n",
      "Epoch 4/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.5898\n",
      "Epoch 4: loss improved from 2.66770 to 2.58985, saving model to weights-improvement-004-2.5898.hdf5\n",
      "1128/1128 [==============================] - 287s 255ms/step - loss: 2.5898\n",
      "Epoch 5/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.5226\n",
      "Epoch 5: loss improved from 2.58985 to 2.52256, saving model to weights-improvement-005-2.5226.hdf5\n",
      "1128/1128 [==============================] - 294s 260ms/step - loss: 2.5226\n",
      "Epoch 6/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.4626\n",
      "Epoch 6: loss improved from 2.52256 to 2.46255, saving model to weights-improvement-006-2.4626.hdf5\n",
      "1128/1128 [==============================] - 292s 259ms/step - loss: 2.4626\n",
      "Epoch 7/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.4092\n",
      "Epoch 7: loss improved from 2.46255 to 2.40921, saving model to weights-improvement-007-2.4092.hdf5\n",
      "1128/1128 [==============================] - 297s 263ms/step - loss: 2.4092\n",
      "Epoch 8/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3576\n",
      "Epoch 8: loss improved from 2.40921 to 2.35756, saving model to weights-improvement-008-2.3576.hdf5\n",
      "1128/1128 [==============================] - 289s 256ms/step - loss: 2.3576\n",
      "Epoch 9/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3093\n",
      "Epoch 9: loss improved from 2.35756 to 2.30934, saving model to weights-improvement-009-2.3093.hdf5\n",
      "1128/1128 [==============================] - 294s 261ms/step - loss: 2.3093\n",
      "Epoch 10/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2668\n",
      "Epoch 10: loss improved from 2.30934 to 2.26679, saving model to weights-improvement-010-2.2668.hdf5\n",
      "1128/1128 [==============================] - 293s 260ms/step - loss: 2.2668\n",
      "Epoch 11/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2242\n",
      "Epoch 11: loss improved from 2.26679 to 2.22422, saving model to weights-improvement-011-2.2242.hdf5\n",
      "1128/1128 [==============================] - 293s 260ms/step - loss: 2.2242\n",
      "Epoch 12/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1853\n",
      "Epoch 12: loss improved from 2.22422 to 2.18533, saving model to weights-improvement-012-2.1853.hdf5\n",
      "1128/1128 [==============================] - 297s 264ms/step - loss: 2.1853\n",
      "Epoch 13/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1458\n",
      "Epoch 13: loss improved from 2.18533 to 2.14583, saving model to weights-improvement-013-2.1458.hdf5\n",
      "1128/1128 [==============================] - 290s 257ms/step - loss: 2.1458\n",
      "Epoch 14/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1115\n",
      "Epoch 14: loss improved from 2.14583 to 2.11146, saving model to weights-improvement-014-2.1115.hdf5\n",
      "1128/1128 [==============================] - 303s 269ms/step - loss: 2.1115\n",
      "Epoch 15/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0801\n",
      "Epoch 15: loss improved from 2.11146 to 2.08006, saving model to weights-improvement-015-2.0801.hdf5\n",
      "1128/1128 [==============================] - 286s 254ms/step - loss: 2.0801\n",
      "Epoch 16/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0448\n",
      "Epoch 16: loss improved from 2.08006 to 2.04479, saving model to weights-improvement-016-2.0448.hdf5\n",
      "1128/1128 [==============================] - 300s 266ms/step - loss: 2.0448\n",
      "Epoch 17/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0169\n",
      "Epoch 17: loss improved from 2.04479 to 2.01688, saving model to weights-improvement-017-2.0169.hdf5\n",
      "1128/1128 [==============================] - 286s 254ms/step - loss: 2.0169\n",
      "Epoch 18/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 1.9879\n",
      "Epoch 18: loss improved from 2.01688 to 1.98788, saving model to weights-improvement-018-1.9879.hdf5\n",
      "1128/1128 [==============================] - 296s 263ms/step - loss: 1.9879\n",
      "Epoch 19/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 1.9606\n",
      "Epoch 19: loss improved from 1.98788 to 1.96063, saving model to weights-improvement-019-1.9606.hdf5\n",
      "1128/1128 [==============================] - 297s 263ms/step - loss: 1.9606\n",
      "Epoch 20/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 1.9360\n",
      "Epoch 20: loss improved from 1.96063 to 1.93596, saving model to weights-improvement-020-1.9360.hdf5\n",
      "1128/1128 [==============================] - 297s 263ms/step - loss: 1.9360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c91be35a60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация текста с помощью сети LSTM\n",
    "Генерация текста с использованием обученной сети LSTM относительно проста.\n",
    "Во-первых, мы загружаем данные и определяем сеть точно таким же образом, за исключением того, что веса сети загружаются из файла контрольных точек, и сеть не нуждается в обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-20-1.9268.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, при подготовке сопоставления уникальных символов с целыми числами мы также должны создать обратное отображение, которое мы можем использовать для преобразования целых чисел обратно в символы, чтобы мы могли понять предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейший способ использования модели Keras LSTM для прогнозирования - сначала начать с последовательности начальных чисел в качестве входных данных, сгенерировать\n",
    "следующий символ, затем обновить последовательность начальных чисел, чтобы добавить сгенерированный символ в конце, и обрезать первый символ. Этот процесс повторяется до тех пор, пока мы хотим предсказать новые символы (например, последовательность длиной 1000 символов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" suddenly that alice had not a moment to think\n",
      "about stopping herself before she found herself fallin \"\n",
      "g hor the sooe of the court, 'a didrter to make to tey the horse oo the soidt to tay in the cousd, and the was sointing to the white rabbit was an in was in the houph, and whst hn th the coumouse thin she was toe oade to the while sabbit was an the could so tee what she was not in the wag oo the had sooe th the court, 'and the more was o was to the woid th the soies so tay in the coutd ' \n",
      "'i moot than yhu ho a corno,s dreieed,' said the dotmouse, ''to the sare thing sas oo the sorpse ' she said to herself, and whin at the could not to the corrt, \n",
      "an cel to see the matter were toie at the could, \n",
      "'the donstes taa io the cortter thing to tou the wou se teln io to ' she said to herself, and whin a little so to the wan so the tooe of the tooe of the gourt, and ths go wo her to ce i vaed to toie that she was no the was oo the wan so the oooe tf the gorrt of the court, 'and the more was o was to the woil th the soiel sf then io an the could ' \n",
      "'the donstes taa io the cortter thing ' said the\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос 2: Написать собственный CallBack, который будет показывать то как генерируется текст во время обучения (то есть раз в какое-то количество эпох генирировать и выводить текст у необученной модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144431\n",
      "Total Vocab:  44\n",
      "Total Patterns:  144391\n",
      "Epoch 1/20\n",
      "282/283 [============================>.] - ETA: 0s - loss: 2.7641\n",
      "Seed:\n",
      ". (as that is rather a hard word, i will\n",
      "Generated text:\n",
      "ktenhiwhiskwatintherans, fere  ofeekil. 'd  akk iee utannehwo 's inisos,t fucind oat ind lom,  alicrit th!lt iisrupgatler thethar, gor wait.  'lit talstgpratn\" ye cou:lous olira.e dy shith. 'wettit, awe daaole-, aadsin tod  eicangopulef ind che owy eid.  we f wit ho geme heooo ty thran, say, sey ahtsemoa y in-malcy hufvwiosihing atsescalasee. t'inwhhanf atmelsy ny witlesiesdon  tffusin? isite heost, on.t te chevebing see the hil!-ead' oreisewan chs boke.   ahe, isit soeer.' aasle thiagat thin he\n",
      "283/283 [==============================] - 117s 409ms/step - loss: 2.7641\n",
      "Epoch 2/20\n",
      "283/283 [==============================] - 93s 330ms/step - loss: 2.2468\n",
      "Epoch 3/20\n",
      "283/283 [==============================] - 94s 333ms/step - loss: 2.0588\n",
      "Epoch 4/20\n",
      "282/283 [============================>.] - ETA: 0s - loss: 1.9280\n",
      "Seed:\n",
      "ense,' said alice more boldly: 'you know\n",
      "Generated text:\n",
      "'st moreking polrty.  'a donrhe han 'it, out leyene to a r, umsed.  belf wnot lave on thi gep u fon?' s    veicprousdved poor hid-qike-.i w he hear; beatoflles it anm!' suigaid offors,' the wartendtry. ede pertew. so afois elrees, no ithe devnoke the evureow. i't, i have ingsung toe rofe, \"in was nage; i's wovet hay of the mortit.'  she har howeed the -app; afise the griytl be tome,  ale chors tee he tuotle,' said theter?'  andist! f owho botcane ase hit eag; you ha couull kidg.  ile sar, a best\n",
      "283/283 [==============================] - 112s 396ms/step - loss: 1.9280\n",
      "Epoch 5/20\n",
      "283/283 [==============================] - 94s 332ms/step - loss: 1.8333\n",
      "Epoch 6/20\n",
      "283/283 [==============================] - 95s 337ms/step - loss: 1.7510\n",
      "Epoch 7/20\n",
      "282/283 [============================>.] - ETA: 0s - loss: 1.6830\n",
      "Seed:\n",
      "sing it as a cushion, resting their elbo\n",
      "Generated text:\n",
      "king shi doon on! ofch, think?  has plemserss, and ie u'tunges: the justerplenxis, laveing in roch the fortid at wey, furxipled deary her adoare. foor wal an a bround alice morking dift!' had the keat woutd to head hou af,'  lick tho gress, fingly waitharse, 'sf yid ad all befuch sart on wist one, in a sanf, a dontured lat be beove of that to the the tits, i ado gut,' she daded on thing, she ofulr!' the pave: 'whand with wist fiched avorg alf expllites.  nat han gain opeoven siyow oo sade.  of, \n",
      "283/283 [==============================] - 115s 407ms/step - loss: 1.6830\n",
      "Epoch 8/20\n",
      "283/283 [==============================] - 95s 336ms/step - loss: 1.6218\n",
      "Epoch 9/20\n",
      "283/283 [==============================] - 95s 335ms/step - loss: 1.5659\n",
      "Epoch 10/20\n",
      "282/283 [============================>.] - ETA: 0s - loss: 1.5138\n",
      "Seed:\n",
      " be rude, so she bore it as well as she \n",
      "Generated text:\n",
      "west tultenthirgs;  poncritber.  'then't nathing mome butie;, and begin she sabbly crossed quite courd bit, just nuver her treally great wett on a acr it off it the brythong! irsull on teroun sonemes a dowaf mack to the courh! 'you, fon that say a moting, 'i saple in i she fan cors.      'it wash i pave a tone the tabbee it,' shi hamped the utearke fort.        \"it oncoused the bect on thies:      \"e,' said the hatper out bechangrin. alice  aup of thoog the kek, shered,' salide looked.) si't lyo\n",
      "283/283 [==============================] - 109s 387ms/step - loss: 1.5138\n",
      "Epoch 11/20\n",
      "283/283 [==============================] - 91s 321ms/step - loss: 1.4725\n",
      "Epoch 12/20\n",
      "283/283 [==============================] - 92s 324ms/step - loss: 1.4302\n",
      "Epoch 13/20\n",
      "282/283 [============================>.] - ETA: 0s - loss: 1.3907\n",
      "Seed:\n",
      "ning with curiosity, she ran across the \n",
      "Generated text:\n",
      "parshors she hed anesed of the expeition down,' she said troughtanx: and they and walked peading a tay, on this tame she would to kyoughtealy you righing, it mouse she were bleaded to the hanterply--'very time by the look of like a gunertoly clatie? \"while shat she hed ducjust to very plait, and she sed much rath timbs and stizled if.'  'armarss,' said the caterpillar;  looking at the reatarker: 'nevertr. the sureabe the marcher--a was you game thin foesning questeass ithing on the house thing m\n",
      "283/283 [==============================] - 109s 384ms/step - loss: 1.3907\n",
      "Epoch 14/20\n",
      "283/283 [==============================] - 93s 329ms/step - loss: 1.3514\n",
      "Epoch 15/20\n",
      "283/283 [==============================] - 92s 324ms/step - loss: 1.3218\n",
      "Epoch 16/20\n",
      "282/283 [============================>.] - ETA: 0s - loss: 1.2882\n",
      "Seed:\n",
      "t doesn't understand english,' thought a\n",
      "Generated text:\n",
      "lice, rehirvent letterccat of the someais; if it quite firse,' said alice.  'i cat us by kitw!' has in lived and hes to gay if it! what woold look it. pether with little picke.  'will you trine yoursed--' as a little schair! i she sad theid some deas! then't know this thing?' said alice, sament: 'the duybed began so poins: i she's through there here to your wan my, porrinew?' said alice to herself, 'the silencw in the leatm\", the eas sone thing!' e-dreadday, thene's all leadne, mast!' mald i'ts \n",
      "283/283 [==============================] - 109s 384ms/step - loss: 1.2882\n",
      "Epoch 17/20\n",
      "283/283 [==============================] - 91s 321ms/step - loss: 1.2621\n",
      "Epoch 18/20\n",
      "283/283 [==============================] - 92s 324ms/step - loss: 1.2343\n",
      "Epoch 19/20\n",
      "282/283 [============================>.] - ETA: 0s - loss: 1.1999\n",
      "Seed:\n",
      "son is,' said the gryphon, 'that they wo\n",
      "Generated text:\n",
      "rks alice feitting: the darcows sacked at anx: but she did not helr \"the forlowed fimwith, he would ask in a votiens hears!' stheim ang, her duniin mystloupcle.'  'there's the earch and or gloss,' they could bely very cortuncing, look all mance.   'si may a congers!' soon whisher them abith itsed some among the last his kious-,' (as he was now, and i could, in a treataes gaid offeraney, and thought it would you swould cair,' said the krech, ancteroutly atatler.  of cousd change, theis alk mock t\n",
      "283/283 [==============================] - 108s 383ms/step - loss: 1.1999\n",
      "Epoch 20/20\n",
      "283/283 [==============================] - 92s 325ms/step - loss: 1.1781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c915c47b80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "\n",
    "filename = \"wonderland.txt\"\n",
    "text = open(filename).read().lower()\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "seq_length = 40\n",
    "\n",
    "n_chars = len(text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(text) - seq_length, 1):\n",
    "    dataX.append(text[i : i + seq_length])\n",
    "    dataY.append(text[i + seq_length])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "x = np.zeros((len(dataX), seq_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(dataX), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sentence in enumerate(dataX):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[dataY[i]]] = 1\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam')\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 3 == 0:\n",
    "            start_index = random.randint(0, len(text) - seq_length - 1)\n",
    "            generated = \"\"\n",
    "            sentence = text[start_index : start_index + seq_length]\n",
    "\n",
    "            print (\"\\nSeed:\")\n",
    "            print (sentence)\n",
    "\n",
    "            print (\"Generated text:\")\n",
    "\n",
    "            for i in range(500):\n",
    "                x_pred = np.zeros((1, seq_length, len(chars)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_to_int[char]] = 1.0\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds)\n",
    "                next_char = int_to_char[next_index]\n",
    "                sentence = sentence[1:] + next_char\n",
    "                generated += next_char\n",
    "\n",
    "            print(generated)\n",
    "\n",
    "model.fit(x, y, epochs=20, batch_size=512, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вопрос 3: Отследить процесс обучения при помощи TensorFlowCallBack (TensorBoard), в отчете привести результаты и их анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144431\n",
      "Total Vocab:  44\n",
      "Total Patterns:  144391\n",
      "Epoch 1/20\n",
      "283/283 [==============================] - 101s 351ms/step - loss: 2.7459\n",
      "Epoch 2/20\n",
      "283/283 [==============================] - 99s 351ms/step - loss: 2.2475\n",
      "Epoch 3/20\n",
      "283/283 [==============================] - 97s 344ms/step - loss: 2.0677\n",
      "Epoch 4/20\n",
      "283/283 [==============================] - 98s 345ms/step - loss: 1.9494\n",
      "Epoch 5/20\n",
      "283/283 [==============================] - 98s 345ms/step - loss: 1.8521\n",
      "Epoch 6/20\n",
      "283/283 [==============================] - 98s 345ms/step - loss: 1.7709\n",
      "Epoch 7/20\n",
      "283/283 [==============================] - 99s 349ms/step - loss: 1.6988\n",
      "Epoch 8/20\n",
      "283/283 [==============================] - 101s 357ms/step - loss: 1.6427\n",
      "Epoch 9/20\n",
      "283/283 [==============================] - 100s 353ms/step - loss: 1.5871\n",
      "Epoch 10/20\n",
      "283/283 [==============================] - 99s 349ms/step - loss: 1.5383\n",
      "Epoch 11/20\n",
      "283/283 [==============================] - 98s 346ms/step - loss: 1.4956\n",
      "Epoch 12/20\n",
      "283/283 [==============================] - 98s 345ms/step - loss: 1.4510\n",
      "Epoch 13/20\n",
      "283/283 [==============================] - 98s 345ms/step - loss: 1.4125\n",
      "Epoch 14/20\n",
      "283/283 [==============================] - 98s 346ms/step - loss: 1.3841\n",
      "Epoch 15/20\n",
      "283/283 [==============================] - 101s 358ms/step - loss: 1.3442\n",
      "Epoch 16/20\n",
      "283/283 [==============================] - 100s 352ms/step - loss: 1.3243\n",
      "Epoch 17/20\n",
      "283/283 [==============================] - 100s 355ms/step - loss: 1.2864\n",
      "Epoch 18/20\n",
      "283/283 [==============================] - 98s 348ms/step - loss: 1.2624\n",
      "Epoch 19/20\n",
      "283/283 [==============================] - 98s 347ms/step - loss: 1.2285\n",
      "Epoch 20/20\n",
      "283/283 [==============================] - 98s 346ms/step - loss: 1.2039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c915bb6f40>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "filename = \"wonderland.txt\"\n",
    "text = open(filename).read().lower()\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "seq_length = 40\n",
    "\n",
    "n_chars = len(text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(text) - seq_length, 1):\n",
    "    dataX.append(text[i : i + seq_length])\n",
    "    dataY.append(text[i + seq_length])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "x = np.zeros((len(dataX), seq_length, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(dataX), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sentence in enumerate(dataX):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[dataY[i]]] = 1\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam')\n",
    "\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0)\n",
    "\n",
    "model.fit(x, y, epochs=20, batch_size=512, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запустим TensorBoard чтобы посмотреть на историю обучения нашей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2476), started 4:00:41 ago. (Use '!kill 2476' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8dceb2ced76438bd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8dceb2ced76438bd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2476), started 4:00:41 ago. (Use '!kill 2476' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e422c103b9c98317\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e422c103b9c98317\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
